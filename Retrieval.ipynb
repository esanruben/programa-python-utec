{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esanruben/programa-python-utec/blob/main/Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3d6f34",
      "metadata": {
        "id": "9a3d6f34"
      },
      "source": [
        "# OllamaEmbeddings\n",
        "## Overview\n",
        "\n",
        "## Setup\n",
        "\n",
        "\n",
        "Primero, sigue [estas instrucciones](https://github.com/jmorganca/ollama) para configurar y ejecutar una instancia local de Ollama:\n",
        "\n",
        "* [Descarga](https://ollama.ai/download) e instala Ollama en las plataformas soportadas (incluyendo Windows Subsystem for Linux)\n",
        "* Obtén un modelo de LLM disponible mediante `ollama pull <nombre-del-modelo>`\n",
        "    * Puedes ver una lista de modelos disponibles en la [biblioteca de modelos](https://ollama.ai/library)\n",
        "    * Por ejemplo: `ollama pull llama3`\n",
        "* Esto descargará la versión etiquetada por defecto del modelo. Normalmente, la versión por defecto apunta al modelo más reciente y de menor tamaño en parámetros.\n",
        "\n",
        "> En Mac, los modelos se descargan en `~/.ollama/models`\n",
        ">\n",
        "> En Linux (o WSL), los modelos se almacenan en `/usr/share/ollama/.ollama/models`\n",
        "\n",
        "* Para especificar una versión exacta del modelo de interés, usa: `ollama pull vicuna:13b-v1.5-16k-q4_0` (Puedes ver las [diferentes etiquetas para el modelo `Vicuna`](https://ollama.ai/library/vicuna/tags) en este caso)\n",
        "* Para ver todos los modelos descargados, usa `ollama list`\n",
        "* Para chatear directamente con un modelo desde la línea de comandos, usa `ollama run <nombre-del-modelo>`\n",
        "* Revisa la [documentación de Ollama](https://github.com/jmorganca/ollama) para más comandos. También puedes ejecutar `ollama help` en la terminal para ver los comandos disponibles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9664366",
      "metadata": {
        "id": "d9664366"
      },
      "source": [
        "### Installation\n",
        "\n",
        "The LangChain Ollama integration lives in the `langchain-ollama` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64853226",
      "metadata": {
        "id": "64853226",
        "outputId": "9fce03f5-56b3-4f73-ed2a-4716729c4bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.11.5 which is incompatible.\n",
            "langchain 0.2.16 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-aws 0.1.6 requires langchain-core<0.3,>=0.1.45, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-community 0.2.7 requires langchain-core<0.3.0,>=0.2.12, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-experimental 0.0.61 requires langchain-core<0.3.0,>=0.2.7, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-groq 0.1.10 requires langchain-core<0.3.0,>=0.2.39, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-openai 0.1.15 requires langchain-core<0.3.0,>=0.2.13, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langchain-text-splitters 0.2.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.3.63 which is incompatible.\n",
            "langgraph 0.1.8 requires langchain-core<0.3,>=0.2.15, but you have langchain-core 0.3.63 which is incompatible.\n",
            "llama-index-core 0.10.46 requires tenacity<8.4.0,>=8.2.0, but you have tenacity 8.5.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dd1724",
      "metadata": {
        "id": "45dd1724"
      },
      "source": [
        "## Instantiation\n",
        "\n",
        "Now we can instantiate our model object and generate embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea7a09b",
      "metadata": {
        "id": "9ea7a09b"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=\"llama3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d271b6",
      "metadata": {
        "id": "77d271b6"
      },
      "source": [
        "## Indexing and Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d817716b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d817716b",
        "outputId": "03bdac97-098b-4a38-eb40-d05dbfba150b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Drying wiring harness\\nIf there is any oil or dirt on the wiring harness,\\nwipe it off with a dry cloth. Avoid washing it in\\nwater or using steam. If  the connector must be\\nwashed in water, do not use high pressure water\\nor steam directly on the wiring harness. If water\\ngets directly on the connector, perform the follow-\\ning procedure.\\na. Disconnect the connector and wipe off the\\nwater with a dry cloth. If the connector is\\nblown dry with compressed air, there is the\\nrisk that oil in the air may cause defective con-\\ntact, so remove all oil and water from the com-\\npressed air before blowing with air.\\nb. If water gets inside the connector, use a dryer\\nto dry the inside of the connector. Hot air from\\nthe dryer can be used, but regulate the time\\nthat the hot air is used in order not to make the\\nconnector or related parts too hot, as this will\\ncause deformation or damage to the connec-\\ntor.\\nc. After drying, leave the wiring harness discon-\\nnected and carry out a continuity test to check\\nfor any short circuits between pins caused by\\nwater.\\nd. After completely drying the connector, blow it\\nwith contact restorer and reassemble.\\n1. Male connector\\n2. Female connector\\na. Lock\\nb. Lock'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a vector store with a sample text\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "text = \"\"\"Drying wiring harness\n",
        "If there is any oil or dirt on the wiring harness,\n",
        "wipe it off with a dry cloth. Avoid washing it in\n",
        "water or using steam. If  the connector must be\n",
        "washed in water, do not use high pressure water\n",
        "or steam directly on the wiring harness. If water\n",
        "gets directly on the connector, perform the follow-\n",
        "ing procedure.\n",
        "a. Disconnect the connector and wipe off the\n",
        "water with a dry cloth. If the connector is\n",
        "blown dry with compressed air, there is the\n",
        "risk that oil in the air may cause defective con-\n",
        "tact, so remove all oil and water from the com-\n",
        "pressed air before blowing with air.\n",
        "b. If water gets inside the connector, use a dryer\n",
        "to dry the inside of the connector. Hot air from\n",
        "the dryer can be used, but regulate the time\n",
        "that the hot air is used in order not to make the\n",
        "connector or related parts too hot, as this will\n",
        "cause deformation or damage to the connec-\n",
        "tor.\n",
        "c. After drying, leave the wiring harness discon-\n",
        "nected and carry out a continuity test to check\n",
        "for any short circuits between pins caused by\n",
        "water.\n",
        "d. After completely drying the connector, blow it\n",
        "with contact restorer and reassemble.\n",
        "1. Male connector\n",
        "2. Female connector\n",
        "a. Lock\n",
        "b. Lock\"\"\"\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_texts(\n",
        "    [text],\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "retrieved_documents = retriever.invoke(\"what happends if water gets inside the connector?\")\n",
        "\n",
        "\n",
        "retrieved_documents[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02b9855",
      "metadata": {
        "id": "e02b9855"
      },
      "source": [
        "### Embed single texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d2befcd",
      "metadata": {
        "id": "0d2befcd",
        "outputId": "e4bbf3f1-baac-4263-d5ba-015bc86be099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0045026396, -0.013362285, 0.0046595125, -0.010100572, 0.04122843, -0.02854382, 0.005660211, 0.003\n"
          ]
        }
      ],
      "source": [
        "single_vector = embeddings.embed_query(text)\n",
        "print(str(single_vector)[:100])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}